{
    "componentChunkName": "component---src-templates-markdown-layout-tsx",
    "path": "/nodejs-crawling/",
    "result": {"data":{"markdownRemark":{"html":"<h2>크롤링(Crawling)</h2>\n<img src=\"https://user-images.githubusercontent.com/71566740/139003885-ded14d60-8ae6-4308-ba25-7bf6375d7e1a.png\" class=\"img large\"/>\n<p>크롤링은 웹 페이지에서 원하는 데이터를 추출해 내는 행위입니다.</p>\n<p>크롤링을 위해 개발된 소프트웨어를 크롤러(Crawler)라 합니다.</p>\n<p><br>저는 보통 API를 만들거나 Tensorflow 학습 데이터를 수집하는데 크롤링을 자주 이용합니다.</p>\n<p>크롤링에 활용 가능한 도구는 언어별로 Jsoup(Java), BeautifulSoup(Python) 등 여러 종류가 있지만 이번 포스트에서는 제가 가장 자주 사용하는 Javascript, Node.js 도구들을 이용하겠습니다.</p>\n<p>(언어마다 도구는 달라도 동작 방식은 대체로 비슷합니다.)</p>\n<p><br><strong>이 포스트에서는 <a href=\"http://openinsider.com/insider-purchases-25k\">OpenInsider</a>(해외 내부자 거래 정보 사이트)를 크롤링 해보겠습니다.</strong></p>\n<h2>도구 선택</h2>\n<p>node.js에서도 크롤링에 사용할 수 있는 도구도 종류가 많고 그중에 용도에 맞는 도구를 선택하면 되겠습니다.</p>\n<p><br>해당 페이지는 로그인도 필요 없고 따로 크롤링이 차단되어 있지도 않기 때문에 단순 http 라이브러리와 parsing 라이브러리만 사용하겠습니다.</p>\n<p><br>만약 특정 이유로 사람이 직접 데이터를 수집하는 것처럼 브라우저를 핸들링하는 방법으로 크롤링 해야 한다면 속도는 느리지만 Chromium을 제어하는 도구들(Puppeteer 등)을 사용하시면 됩니다.\r\n<br>(시간이 된다면 Puppeteer 사용방법도 다뤄보도록 하겠습니다.)</p>\n<h3>📚 Node.js 라이브러리</h3>\n<h4>HTTP 라이브러리: Axios</h4>\n<ul>\n<li>http 라이브러리에는 종류가 굉장히 많고 저는 평소 Request를 자주 사용해 왔는데 해당 라이브러리가 deprecated 되었다는 소식을 듣고 성능이 좋다는 Axios를 사용해 보기로 했습니다.</li>\n</ul>\n<h4>Parsing 라이브러리: Cheerio</h4>\n<ul>\n<li>사실 parsing 라이브러리는 없어도 직접 파싱 해서 사용할 수 있지만 방대한 량의 html 코드를 파싱 하는 과정이 복잡해질뿐더러 코드의 가독성도 떨어집니다.\r\n<br>저는 여기서 jQuery 문법을 그대로 사용할 수 있는 Cheerio를 사용해서 파싱 하겠습니다.</li>\n</ul>\n<h2>개발환경 설정</h2>\n<h3>새 프로젝트 생성</h3>\n<deckgo-highlight-code language=\"bash\"  >\n          <code slot=\"code\">$ mkdir &lt;프로젝트 이름&gt;\r\n$ cd &lt;프로젝트 이름&gt;\r\n$ npm init</code>\n        </deckgo-highlight-code>\n<h3>사용할 node.js 모듈 설치</h3>\n<deckgo-highlight-code language=\"bash\"  >\n          <code slot=\"code\">npm install axios cheerio</code>\n        </deckgo-highlight-code>\n<h2>크롤링 맛보기</h2>\n<p>이제 원하는 정보들의 위치를 찾아야 합니다.\r\n<br>원하는 정보를 오른쪽 마우스로 클릭후 검사를 사용하면 쉽게 찾을 수 있습니다.\r\n<br>저는 거래 날짜와 해당 주식의 ticker 값을 받아오기 위한 selector를 복사해보겠습니다.</p>\n<img src=\"https://user-images.githubusercontent.com/71566740/131478329-82d599e6-56fa-44df-b80a-e609896315f8.png\" class=\"img large\"/>\n<p>첫번째 값을 기준으로 각 정보의 slector는 다음과 같은걸 확인 할 수 있습니다.</p>\n<p><br><strong>거래날짜</strong>: <code>#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(3) > div</code>\r\n<br><strong>ticker</strong>: <code>#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(4) > b > a</code></p>\n<p><br>이 구조를 보면 한 가지 거래 정보는 같은 tr에 포함되어 있다는 걸 알 수 있습니다.</p>\n<h3><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>JavaScript</title><path d=\"M0 0h24v24H0V0zm22.034 18.276c-.175-1.095-.888-2.015-3.003-2.873-.736-.345-1.554-.585-1.797-1.14-.091-.33-.105-.51-.046-.705.15-.646.915-.84 1.515-.66.39.12.75.42.976.9 1.034-.676 1.034-.676 1.755-1.125-.27-.42-.404-.601-.586-.78-.63-.705-1.469-1.065-2.834-1.034l-.705.089c-.676.165-1.32.525-1.71 1.005-1.14 1.291-.811 3.541.569 4.471 1.365 1.02 3.361 1.244 3.616 2.205.24 1.17-.87 1.545-1.966 1.41-.811-.18-1.26-.586-1.755-1.336l-1.83 1.051c.21.48.45.689.81 1.109 1.74 1.756 6.09 1.666 6.871-1.004.029-.09.24-.705.074-1.65l.046.067zm-8.983-7.245h-2.248c0 1.938-.009 3.864-.009 5.805 0 1.232.063 2.363-.138 2.711-.33.689-1.18.601-1.566.48-.396-.196-.597-.466-.83-.855-.063-.105-.11-.196-.127-.196l-1.825 1.125c.305.63.75 1.172 1.324 1.517.855.51 2.004.675 3.207.405.783-.226 1.458-.691 1.811-1.411.51-.93.402-2.07.397-3.346.012-2.054 0-4.109 0-6.179l.004-.056z\"/></svg>index.js</h3>\n<p>생성한 프로젝트에 index.js 파일을 만들고 코드를 작성합니다.\r\n<br>여러 가지 방법이 있겠지만 저는 map 메서드를 이용해 모든 tr들의 거래 날짜와 정보를 가진 객체 배열을 출력하는 코드로 작성했습니다.</p>\n<deckgo-highlight-code language=\"javascript\"  >\n          <code slot=\"code\">const cheerio = require(&quot;cheerio&quot;);\r\nconst axios = require(&quot;axios&quot;);\r\n\r\n(async () =&gt; {\r\n  //크롤링 대상 URL, axios의 get은 비동기 함수이므로 async-await을 사용한다.\r\n  const html = await axios.get(&quot;http://openinsider.com/insider-purchases-25k&quot;),\r\n    $ = cheerio.load(html.data);\r\n\r\n  const trElements = $(&quot;#tablewrapper &gt; table &gt; tbody &gt; tr&quot;);\r\n  const insiderTradeData = trElements\r\n    .map((index, tr) =&gt; ({\r\n      date: $(tr).find(&quot;td:nth-child(3) &gt; div&quot;).text(),\r\n      ticker: $(tr).find(&quot;td:nth-child(4) &gt; b &gt; a&quot;).text(),\r\n    }))\r\n    .toArray();\r\n  console.log(insiderTradeData);\r\n})();</code>\n        </deckgo-highlight-code>\n<h3><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Node.js</title><path d=\"M11.998,24c-0.321,0-0.641-0.084-0.922-0.247l-2.936-1.737c-0.438-0.245-0.224-0.332-0.08-0.383 c0.585-0.203,0.703-0.25,1.328-0.604c0.065-0.037,0.151-0.023,0.218,0.017l2.256,1.339c0.082,0.045,0.197,0.045,0.272,0l8.795-5.076 c0.082-0.047,0.134-0.141,0.134-0.238V6.921c0-0.099-0.053-0.192-0.137-0.242l-8.791-5.072c-0.081-0.047-0.189-0.047-0.271,0 L3.075,6.68C2.99,6.729,2.936,6.825,2.936,6.921v10.15c0,0.097,0.054,0.189,0.139,0.235l2.409,1.392 c1.307,0.654,2.108-0.116,2.108-0.89V7.787c0-0.142,0.114-0.253,0.256-0.253h1.115c0.139,0,0.255,0.112,0.255,0.253v10.021 c0,1.745-0.95,2.745-2.604,2.745c-0.508,0-0.909,0-2.026-0.551L2.28,18.675c-0.57-0.329-0.922-0.945-0.922-1.604V6.921 c0-0.659,0.353-1.275,0.922-1.603l8.795-5.082c0.557-0.315,1.296-0.315,1.848,0l8.794,5.082c0.57,0.329,0.924,0.944,0.924,1.603 v10.15c0,0.659-0.354,1.273-0.924,1.604l-8.794,5.078C12.643,23.916,12.324,24,11.998,24z M19.099,13.993 c0-1.9-1.284-2.406-3.987-2.763c-2.731-0.361-3.009-0.548-3.009-1.187c0-0.528,0.235-1.233,2.258-1.233 c1.807,0,2.473,0.389,2.747,1.607c0.024,0.115,0.129,0.199,0.247,0.199h1.141c0.071,0,0.138-0.031,0.186-0.081 c0.048-0.054,0.074-0.123,0.067-0.196c-0.177-2.098-1.571-3.076-4.388-3.076c-2.508,0-4.004,1.058-4.004,2.833 c0,1.925,1.488,2.457,3.895,2.695c2.88,0.282,3.103,0.703,3.103,1.269c0,0.983-0.789,1.402-2.642,1.402 c-2.327,0-2.839-0.584-3.011-1.742c-0.02-0.124-0.126-0.215-0.253-0.215h-1.137c-0.141,0-0.254,0.112-0.254,0.253 c0,1.482,0.806,3.248,4.655,3.248C17.501,17.007,19.099,15.91,19.099,13.993z\"/></svg>실행</h3>\n<p><code>node index</code>로 실행해보면 결과는 다음과 같이 크롤링된 정보들을 확인할 수 있습니다.</p>\n<img src=\"https://user-images.githubusercontent.com/71566740/133531609-93363fba-e51e-47aa-b03a-5cad03bc1795.png\" class=\"img large\"/>\n<h2>마지막으로</h2>\n<ul>\n<li>위에서 사용했던 http 라이브러리나 parsing 라이브러리 둘 다 크롤링에 활용 가능한 도구이지 '크롤링만을 위한 도구'가 아닙니다.\r\n<br>해당 라이브러리가 어떤 역할을 하는지 직접 찾아보시는 것도 좋을 것 같습니다.</li>\n<li>크롤링은 꼭 개발자가 아니더라도 원하는 데이터를 자급자족할 수 있다는 점에서 활용도가 정말 높습니다.\r\n<br>위에서 다뤘던 예제는 기초적인 크롤링 방법만을 다룬 것이므로 실제 크롤링을 이용해 무언가를 하려면 대상의 URL 규칙성, 페이지의 구조 등을 직접 분석해보면서 코드를 작성하는 방법을 고민해 보셔야 합니다.</li>\n<li>크롤링은 사람이 직접 데이터를 수집하는 것보다 훨씬 빠른 속도로 서버에 다수에 요청을 보내서 데이터를 응답받기 때문에 크롤링 대상 서버에 문제를 발생시킬 수 있습니다.\r\n<br> 위와 같은 이유로 크롤링이 차단되어 있는 사이트들도 있으니 크롤링이 허용되어 있는 사이트인지 확인하는 것도 중요합니다.</li>\n<li>크롤링으로 처벌을 받은 판례가 있으니 실제 서비스를 위한 코드를 작성할 때는 해당 정보가 크롤링이 허용되는 정보인지 잘 확인해 보시고 사용하는 게 좋을 것 같습니다.</li>\n</ul>","frontmatter":{"emoji":"📢","title":"크롤링을 통한 데이터 수집하기","date":"2021-08-31","description":"Node.js 크롤링을 통한 데이터 수집","tag":["Javascript","Data"]},"fields":{"slug":"/nodejs-crawling/"},"id":"2a3b304d-fff2-5587-9a14-4f3a1864ba1d"},"allMarkdownRemark":{"nodes":[{"fields":{"slug":"/korea-webtoon-api-update/"},"frontmatter":{"description":"Daum 웹툰의 대규모 변화로 인한 API 업데이트","title":"Korea Webtoon API 업데이트"},"id":"70e1cf28-97c5-594c-ab1d-16d4ec69657c"},{"fields":{"slug":"/nest-js/"},"frontmatter":{"description":"Typescript를 이용한 NestJS 코드 작성","title":"NestJS 웹서버 구축하기"},"id":"305141a9-5929-57a3-aeed-d6841c809e43"},{"fields":{"slug":"/knu-lms-scheduler/"},"frontmatter":{"description":"공주대학교 비대면 강의 시스템 UI 개선 프로젝트","title":"KNU LMS Scheduler 프로젝트"},"id":"c650e111-86f5-5201-b9f9-d40fccf2a8eb"},{"fields":{"slug":"/gititle-project/"},"frontmatter":{"description":"좋은 커밋 메시지를 작성하기 위한 프로젝트","title":"Gititle 프로젝트"},"id":"59da09b0-cfe8-5387-8b2c-18728bc48eb5"},{"fields":{"slug":"/sass-compiler/"},"frontmatter":{"description":"편리한 Sass 사용을 위한 익스텐션","title":"Live Sass Compiler 사용하기"},"id":"cddd5b1b-eb1c-5a3a-bf59-e59f20abc909"},{"fields":{"slug":"/nodejs-crawling/"},"frontmatter":{"description":"Node.js 크롤링을 통한 데이터 수집","title":"크롤링을 통한 데이터 수집하기"},"id":"2a3b304d-fff2-5587-9a14-4f3a1864ba1d"},{"fields":{"slug":"/copilot-review/"},"frontmatter":{"description":"Github의 코딩 AI, Copilot 한 달 사용 리뷰","title":"Copilot 사용 후기"},"id":"8a284f98-aa79-5a11-9f60-17da05b03882"},{"fields":{"slug":"/typescript-express/"},"frontmatter":{"description":"Typescript를 이용한 Express 코드 작성","title":"Typescript + Express 웹서버 구축하기"},"id":"7de321b0-d894-5d5a-a7e1-058f7c309364"},{"fields":{"slug":"/blog-remake-review1/"},"frontmatter":{"description":"해당 블로그 제작 중간 리뷰","title":"Next.js에서 Gatsby로 블로그 이사 중간 리뷰"},"id":"4cfa4a82-589b-5649-8fd0-5cbb6f48d40b"},{"fields":{"slug":"/good-commit-message/"},"frontmatter":{"description":"Commit Message 작성을 위한 나와의 약속","title":"Git Commit Message Convention"},"id":"98992a08-a70f-5f2e-b087-f3ddf6331f1d"}]}},"pageContext":{"slug":"/nodejs-crawling/"}},
    "staticQueryHashes": []}